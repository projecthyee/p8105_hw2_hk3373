---
title: "P8105 Homework 2"
author: "Hyun Kim (hk3373)"
date: "`r Sys.Date()`"
output: github_document
---

``` {r setup, echo = FALSE, message = FALSE}
library(tidyverse)
```

# Problem 1

## Import and tidy NYC transit dataset:
```{r import_transit_data}

nyc_transit_df = 
  read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
           na = "",
           col_types = cols(
             `Route8` = col_character(),
             `Route9` = col_character(),
             `Route10` = col_character(),
             `Route11` = col_character()
           )) |>
  janitor::clean_names() |>
  select(line:entry, vending, ada) |>
  mutate(entry = 
           case_match(
             entry,
             "YES" ~ TRUE,
             "NO" ~ FALSE),
         entry = as.logical(entry))

str(nyc_transit_df)

```

The dataset includes transit line, station name, latitude, longitude, routes 
(from 1 to 11), entrance type, entry, vending and ada compliance. The station's 
latitude and longitude are numeric variables, while entry and ada compliance are
logical variables. The other variables, such as line, station name, entrance 
type, vending and route1 to route11 are character variables. 

For the data cleaning process, NA values were defined when reading the csv file, 
and route8 to route11 were converted to character variables for consistency 
since route1 to route7 were character variables. The dataset has a dimension of 
`r nrow(nyc_transit_df)` rows and `r ncol(nyc_transit_df)` columns. The dataset 
is not tidy since route number should be a variable including the values of all 
routes, which can be resolved using the pivot_longer function.

### How many distinct stations are there? 
```{r compute_distinct_stations}
num_station = nrow(nyc_transit_df |> 
                      distinct(line, station_name))
```
There are `r num_station` distinct stations.

### How many stations are ADA compliant?
```{r compute_ada_compliant} 
num_ada = nrow(nyc_transit_df |> 
                 filter(ada==TRUE) |> 
                 distinct(line, station_name))
```
`r num_ada` stations are ADA compliant. 

### What proportion of station entrances / exits without vending allow entrance?
```{r compute_proportion_entrance}
prop_entrance = round(
  nrow(nyc_transit_df |> 
          filter(vending == "NO") |> 
          filter(entry == TRUE)) / 
     nrow(nyc_transit_df |>
            filter(vending == "NO")) * 100, 2)
```
`r prop_entrance` percent of station entrances/exits without vending allow 
entrance. 

## Reformat transit data:
```{r reformat_transit_data}

nyc_transit_df = nyc_transit_df |>
  pivot_longer(
    route1:route11,
    names_to = "route_number",
    values_to = "route_name"
  ) |>
  relocate(route_number, route_name) |>
  drop_na()

```

### How many distinct stations serve the A train? 
```{r compute_distinct_A} 
num_stations_A = nrow(nyc_transit_df |> 
                          filter(route_name == "A") |> 
                          distinct(line, station_name))
```
`r num_stations_A` distinct stations serve the A train. 

### Of the stations that serve the A train, how many are ADA compliant?
```{r compute_ada_A}
num_ada_A = nrow(nyc_transit_df |> 
                         filter(route_name == "A") |> 
                         filter(ada == TRUE) |> distinct(line, station_name))
```
`r num_ada_A` of the stations that serve the A train are ADA compliant.

# Problem 2

## Import, tidy and merge Trash Wheel datasets:
```{r import_merge_trash_data}

mr_trash_df = 
  readxl::read_excel(
    "./data/202409 Trash Wheel Collection Data.xlsx",
    sheet = "Mr. Trash Wheel",
    range = "A2:N653",
    na = "") |>
  janitor::clean_names() |>
  mutate(
    year = as.numeric(year),
    sports_balls = as.integer(round(sports_balls)),
    trash_wheel = "Mr.") |>
  relocate(trash_wheel)

prof_trash_df = 
  readxl::read_excel(
    "./data/202409 Trash Wheel Collection Data.xlsx",
    sheet = "Professor Trash Wheel",
    range = "A2:M120",
    na = "") |>
  janitor::clean_names() |>
  mutate(
    year = as.numeric(year),
    sports_balls = NA,
    trash_wheel = "Professor") |>
  relocate(trash_wheel)

gwynnda_trash_df = 
  readxl::read_excel(
    "./data/202409 Trash Wheel Collection Data.xlsx",
    sheet = "Gwynnda Trash Wheel",
    range = "A2:L265") |>
  janitor::clean_names() |>
  mutate(
    year = as.numeric(year),
    sports_balls = NA,
    glass_bottles = NA,
    trash_wheel = "Gwynnda") |>
  relocate(trash_wheel)

merged_trash_df = rbind(mr_trash_df, prof_trash_df, gwynnda_trash_df)
str(merged_trash_df)

```

The dataset has `r nrow(merged_trash_df)` rows and `r ncol(merged_trash_df)` 
columns. Important variables include the weight (tons) and volume (cubic yards) 
of trash collected, date and homes powered. The dataset also includes the types 
and respective quantities of trash collected, such as plastic bottles, 
polystyrene, cigarette butts, glass bottles, plastic bags, wrappers and sports 
balls. 

The total trash collected by Mr. Trash Wheel, Professor Trash Wheel and Gwynnda 
is `r round(sum(pull(merged_trash_df, weight_tons)), 2)` tons and 
`r round(sum(pull(merged_trash_df, volume_cubic_yards)), 2)` cubic yards, powering 
a total of `r round(sum(pull(merged_trash_df, homes_powered), na.rm=TRUE))` homes. 

### What was the total weight of trash collected by Professor Trash Wheel?
```{r professor_trash_weight}
prof_total_trash = sum(merged_trash_df |> 
                              filter(trash_wheel == "Professor") |> 
                              pull(weight_tons))
```
Professor Trash Wheel collected `r prof_total_trash` tons of trash in total. 

### What was the total number of cigarette butts collected by Gwynnda in June of 2022?
```{r}
gwynnda_june_cigs = sum(merged_trash_df |> 
                           filter(trash_wheel == "Gwynnda") |> 
                           filter(month == "June") |> 
                           filter(year == 2022) |> 
                           pull(cigarette_butts))
```
Gwynnda collected `r gwynnda_june_cigs` cigarette butts in June of 2022. 

# Problem 3

## Import and tidy Great British Bake Off (GBB) datasets:
``` {r import_gbb_data}

bakers_df = 
  read_csv("./data/gbb_datasets/bakers.csv") |>
  janitor::clean_names() |>
  separate(baker_name, into = c("baker", "last_name"), sep = " ") |>
  mutate(
    baker = replace(baker, baker == "Jo", "Joanne")
  )

bakes_df = 
  read_csv(
    "./data/gbb_datasets/bakes.csv",
    na = c("N/A", "UNKNOWN", "Unknown")) |>
  janitor::clean_names() |>
  mutate(
    baker = replace(baker, baker == '"Jo"', "Joanne")
  )

results_df = 
  read_csv(
    "./data/gbb_datasets/results.csv",
    na = c("NA", ""), 
    skip = 2) |>
  janitor::clean_names()

``` 

After examining all the datasets, the NA values were defined when reading the 
csv files and column names were cleaned using the clean_names function of the 
janitor library. There is a contestant named Joanne, who was represented 
differently as Jo and "Jo" in the bakers and bakes datasets respectively. Those 
values were subsequently replaced to Joanne for consistency.

When reading the csv of the results dataset, the first two lines were skipped 
since they didn't contain relevant information. For the bakers dataset, the 
baker_name variable was separated into two distinct columns, baker and last_name 
since we need to merge all three datasets, given that the bakes and results 
datasets only included the the bakers' first names in a column called baker. 

## Merge and export GBB datasets:
```{r merge_export_gbb}

merged_gbb_df = 
  left_join(results_df, bakes_df, 
            by = join_by(series, episode, baker)) |>
  left_join(bakers_df, by = join_by(series, baker)) |> 
  relocate(last_name, .after = baker) |>
  relocate(signature_bake, .before = technical) |>
  relocate(show_stopper, .before = result)

anti_join(results_df, merged_gbb_df,
          by = join_by(series, episode, baker, technical, result))

anti_join(bakes_df, merged_gbb_df,
          by = join_by(series, episode, baker, signature_bake,show_stopper))

anti_join(bakers_df, merged_gbb_df, 
          by = join_by(baker, last_name, series, 
                       baker_age,baker_occupation, hometown))

write.csv(merged_gbb_df, "./data/gbb_datasets/merged_gbb.csv", row.names = FALSE)
str(merged_gbb_df)
```

After merging the datasets, check for completeness and correctness was performed 
by using the anti_join function on each of the dataset with the merged dataset. 
As shown above, each of the anti_join function returned an empty dataframe, 
indicating that none of the data were omitted. 

The final dataset has `r nrow(merged_gbb_df)` rows and `r ncol(merged_gbb_df)` 
columns. It includes the series, episode, technical and baker age as numerical 
variables with baker (first name), last name, signature bake, show stopper, 
occupation and hometown as character variables. 

## Create star baker table:
```{r create_starbaker_dataframe}

star_baker_table = merged_gbb_df |>
  filter(result %in% c("STAR BAKER", "WINNER")) |>
  filter(series %in% c(5:10)) |> 
  arrange(series, episode) |>
  select(episode, baker, series) |>
  pivot_wider(
    names_from = "series",
    values_from = "baker"  ) |>
  rename("series 5" = "5",
         "series 6" = "6",
         "series 7" = "7",
         "series 8" = "8",
         "series 9" = "9",
         "series 10" = "10") |>
  knitr::kable()

star_baker_table

```

### Were there any predictable overall winners? Any surprises?

The following contestants had the most titles for star baker before episode 10 
(the final episode) and were predicted to win:

* Richard (3 times for series 5)
* Either Ian or Nadiya (3 times each for series 6), 
* Candice (3 times for series 7)
* Steven (3 times for series 8)
* Either Rahul, Kim-Joy or Ruby (2 times each for series 9)
* Steph (4 times for series 10).

As shown in the table, for series 6, 7 and 9, Nadiya, Candice and Rahul won 
respectively. As a surprise, Nancy, Sophie, and David won series 5, 8, and 10 
respectively. 

## Import and tidy viewership dataset:
```{r import_viewership_data}

viewership_df =
  read_csv("./data/gbb_datasets/viewers.csv",
           na = "NA") |>
  janitor::clean_names()

head(viewership_df, 10)

```

### What was the average viewership in Season 1? In Season 5?
The average viewership of seasons 1 and 5 were 
`r mean(pull(viewership_df,series_1), na.rm = TRUE)` and 
`r round(mean(pull(viewership_df,series_5), na.rm = TRUE), 2)` respectively.
