P8105 Homework 2
================
Hyun Kim (hk3373)
2024-10-02

# Problem 1

## Import and tidy NYC transit dataset:

``` r
nyc_transit_df = 
  read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
           na = "",
           col_types = cols(
             `Route8` = col_character(),
             `Route9` = col_character(),
             `Route10` = col_character(),
             `Route11` = col_character()
           )) |>
  janitor::clean_names() |>
  select(line:entry, vending, ada) |>
  mutate(entry = 
           case_match(
             entry,
             "YES" ~ TRUE,
             "NO" ~ FALSE),
         entry = as.logical(entry))

str(nyc_transit_df)
```

    ## tibble [1,868 × 19] (S3: tbl_df/tbl/data.frame)
    ##  $ line             : chr [1:1868] "4 Avenue" "4 Avenue" "4 Avenue" "4 Avenue" ...
    ##  $ station_name     : chr [1:1868] "25th St" "25th St" "36th St" "36th St" ...
    ##  $ station_latitude : num [1:1868] 40.7 40.7 40.7 40.7 40.7 ...
    ##  $ station_longitude: num [1:1868] -74 -74 -74 -74 -74 ...
    ##  $ route1           : chr [1:1868] "R" "R" "N" "N" ...
    ##  $ route2           : chr [1:1868] NA NA "R" "R" ...
    ##  $ route3           : chr [1:1868] NA NA NA NA ...
    ##  $ route4           : chr [1:1868] NA NA NA NA ...
    ##  $ route5           : chr [1:1868] NA NA NA NA ...
    ##  $ route6           : chr [1:1868] NA NA NA NA ...
    ##  $ route7           : chr [1:1868] NA NA NA NA ...
    ##  $ route8           : chr [1:1868] NA NA NA NA ...
    ##  $ route9           : chr [1:1868] NA NA NA NA ...
    ##  $ route10          : chr [1:1868] NA NA NA NA ...
    ##  $ route11          : chr [1:1868] NA NA NA NA ...
    ##  $ entrance_type    : chr [1:1868] "Stair" "Stair" "Stair" "Stair" ...
    ##  $ entry            : logi [1:1868] TRUE TRUE TRUE TRUE TRUE TRUE ...
    ##  $ vending          : chr [1:1868] "YES" "YES" "YES" "YES" ...
    ##  $ ada              : logi [1:1868] FALSE FALSE FALSE FALSE FALSE FALSE ...

The dataset includes transit line, station name, latitude, longitude,
routes (from 1 to 11), entrance type, entry, vending and ada compliance.
The station’s latitude and longitude are numeric variables, while entry
and ada compliance are logical variables. The other variables, such as
line, station name, entrance type, vending and route1 to route11 are
character variables.

For the data cleaning process, NA values were defined when reading the
csv file, and route8 to route11 were converted to character variables
for consistency since route1 to route7 were character variables. The
dataset has a dimension of 1868 rows and 19 columns. The dataset is not
tidy since route number should be a variable including the values of all
routes, which can be resolved using the pivot_longer function.

### How many distinct stations are there?

``` r
num_station = nrow(nyc_transit_df |> 
                      distinct(line, station_name))
```

There are 465 distinct stations.

### How many stations are ADA compliant?

``` r
num_ada = nrow(nyc_transit_df |> 
                 filter(ada==TRUE) |> 
                 distinct(line, station_name))
```

84 stations are ADA compliant.

### What proportion of station entrances / exits without vending allow entrance?

``` r
prop_entrance = round(
  nrow(nyc_transit_df |> 
          filter(vending == "NO") |> 
          filter(entry == TRUE)) / 
     nrow(nyc_transit_df |>
            filter(vending == "NO")) * 100, 2)
```

37.7 percent of station entrances/exits without vending allow entrance.

## Reformat transit data:

``` r
nyc_transit_df = nyc_transit_df |>
  pivot_longer(
    route1:route11,
    names_to = "route_number",
    values_to = "route_name"
  ) |>
  relocate(route_number, route_name) |>
  drop_na()
```

### How many distinct stations serve the A train?

``` r
num_stations_A = nrow(nyc_transit_df |> 
                          filter(route_name == "A") |> 
                          distinct(line, station_name))
```

60 distinct stations serve the A train.

### Of the stations that serve the A train, how many are ADA compliant?

``` r
num_ada_A = nrow(nyc_transit_df |> 
                         filter(route_name == "A") |> 
                         filter(ada == TRUE) |> distinct(line, station_name))
```

17 of the stations that serve the A train are ADA compliant.

# Problem 2

## Import, tidy and merge Trash Wheel datasets:

``` r
mr_trash_df = 
  readxl::read_excel(
    "./data/202409 Trash Wheel Collection Data.xlsx",
    sheet = "Mr. Trash Wheel",
    range = "A2:N653",
    na = "") |>
  janitor::clean_names() |>
  mutate(
    year = as.numeric(year),
    sports_balls = as.integer(round(sports_balls)),
    trash_wheel = "Mr.") |>
  relocate(trash_wheel)

prof_trash_df = 
  readxl::read_excel(
    "./data/202409 Trash Wheel Collection Data.xlsx",
    sheet = "Professor Trash Wheel",
    range = "A2:M120",
    na = "") |>
  janitor::clean_names() |>
  mutate(
    year = as.numeric(year),
    sports_balls = NA,
    trash_wheel = "Professor") |>
  relocate(trash_wheel)

gwynnda_trash_df = 
  readxl::read_excel(
    "./data/202409 Trash Wheel Collection Data.xlsx",
    sheet = "Gwynnda Trash Wheel",
    range = "A2:L265") |>
  janitor::clean_names() |>
  mutate(
    year = as.numeric(year),
    sports_balls = NA,
    glass_bottles = NA,
    trash_wheel = "Gwynnda") |>
  relocate(trash_wheel)

merged_trash_df = rbind(mr_trash_df, prof_trash_df, gwynnda_trash_df)
str(merged_trash_df)
```

    ## tibble [1,032 × 15] (S3: tbl_df/tbl/data.frame)
    ##  $ trash_wheel       : chr [1:1032] "Mr." "Mr." "Mr." "Mr." ...
    ##  $ dumpster          : num [1:1032] 1 2 3 4 5 6 7 8 9 10 ...
    ##  $ month             : chr [1:1032] "May" "May" "May" "May" ...
    ##  $ year              : num [1:1032] 2014 2014 2014 2014 2014 ...
    ##  $ date              : POSIXct[1:1032], format: "2014-05-16" "2014-05-16" ...
    ##  $ weight_tons       : num [1:1032] 4.31 2.74 3.45 3.1 4.06 2.71 1.91 3.7 2.52 3.76 ...
    ##  $ volume_cubic_yards: num [1:1032] 18 13 15 15 18 13 8 16 14 18 ...
    ##  $ plastic_bottles   : num [1:1032] 1450 1120 2450 2380 980 1430 910 3580 2400 1340 ...
    ##  $ polystyrene       : num [1:1032] 1820 1030 3100 2730 870 2140 1090 4310 2790 1730 ...
    ##  $ cigarette_butts   : num [1:1032] 126000 91000 105000 100000 120000 90000 56000 112000 98000 130000 ...
    ##  $ glass_bottles     : num [1:1032] 72 42 50 52 72 46 32 58 49 75 ...
    ##  $ plastic_bags      : num [1:1032] 584 496 1080 896 368 ...
    ##  $ wrappers          : num [1:1032] 1162 874 2032 1971 753 ...
    ##  $ sports_balls      : int [1:1032] 7 5 6 6 7 5 3 6 6 7 ...
    ##  $ homes_powered     : num [1:1032] 0 0 0 0 0 0 0 0 0 0 ...

The dataset has 1032 rows and 15 columns. Important variables include
the weight (tons) and volume (cubic yards) of trash collected, date and
homes powered. The dataset also includes the types and respective
quantities of trash collected, such as plastic bottles, polystyrene,
cigarette butts, glass bottles, plastic bags, wrappers and sports balls.

The total trash collected by Mr. Trash Wheel, Professor Trash Wheel and
Gwynnda is 3135.47 tons and 1.5559^{4} cubic yards, powering a total of
4.4822^{4} homes.

### What was the total weight of trash collected by Professor Trash Wheel?

``` r
prof_total_trash = sum(merged_trash_df |> 
                              filter(trash_wheel == "Professor") |> 
                              pull(weight_tons))
```

Professor Trash Wheel collected 246.74 tons of trash in total.

### What was the total number of cigarette butts collected by Gwynnda in June of 2022?

``` r
gwynnda_june_cigs = sum(merged_trash_df |> 
                           filter(trash_wheel == "Gwynnda") |> 
                           filter(month == "June") |> 
                           filter(year == 2022) |> 
                           pull(cigarette_butts))
```

Gwynnda collected 1.812^{4} cigarette butts in June of 2022.

# Problem 3

## Import and tidy Great British Bake Off (GBB) datasets:

``` r
bakers_df = 
  read_csv("./data/gbb_datasets/bakers.csv") |>
  janitor::clean_names() |>
  separate(baker_name, into = c("baker", "last_name"), sep = " ") |>
  mutate(
    baker = replace(baker, baker == "Jo", "Joanne")
  )
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
bakes_df = 
  read_csv(
    "./data/gbb_datasets/bakes.csv",
    na = c("N/A", "UNKNOWN", "Unknown")) |>
  janitor::clean_names() |>
  mutate(
    baker = replace(baker, baker == '"Jo"', "Joanne")
  )
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
results_df = 
  read_csv(
    "./data/gbb_datasets/results.csv",
    na = c("NA", ""), 
    skip = 2) |>
  janitor::clean_names()
```

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

After examining all the datasets, the NA values were defined when
reading the csv files and column names were cleaned using the
clean_names function of the janitor library. There is a contestant named
Joanne, who was represented differently as Jo and “Jo” in the bakers and
bakes datasets respectively. Those values were subsequently replaced to
Joanne for consistency.

When reading the csv of the results dataset, the first two lines were
skipped since they didn’t contain relevant information. For the bakers
dataset, the baker_name variable was separated into two distinct
columns, baker and last_name since we need to merge all three datasets,
given that the bakes and results datasets only included the the bakers’
first names in a column called baker.

## Merge and export GBB datasets:

``` r
merged_gbb_df = 
  left_join(results_df, bakes_df, 
            by = join_by(series, episode, baker)) |>
  left_join(bakers_df, by = join_by(series, baker)) |> 
  relocate(last_name, .after = baker) |>
  relocate(signature_bake, .before = technical) |>
  relocate(show_stopper, .before = result)

anti_join(results_df, merged_gbb_df,
          by = join_by(series, episode, baker, technical, result))
```

    ## # A tibble: 0 × 5
    ## # ℹ 5 variables: series <dbl>, episode <dbl>, baker <chr>, technical <dbl>,
    ## #   result <chr>

``` r
anti_join(bakes_df, merged_gbb_df,
          by = join_by(series, episode, baker, signature_bake,show_stopper))
```

    ## # A tibble: 0 × 5
    ## # ℹ 5 variables: series <dbl>, episode <dbl>, baker <chr>,
    ## #   signature_bake <chr>, show_stopper <chr>

``` r
anti_join(bakers_df, merged_gbb_df, 
          by = join_by(baker, last_name, series, 
                       baker_age,baker_occupation, hometown))
```

    ## # A tibble: 0 × 6
    ## # ℹ 6 variables: baker <chr>, last_name <chr>, series <dbl>, baker_age <dbl>,
    ## #   baker_occupation <chr>, hometown <chr>

``` r
write.csv(merged_gbb_df, "merged_gbb.csv", row.names = FALSE)
str(merged_gbb_df)
```

    ## tibble [1,136 × 11] (S3: tbl_df/tbl/data.frame)
    ##  $ series          : num [1:1136] 1 1 1 1 1 1 1 1 1 1 ...
    ##  $ episode         : num [1:1136] 1 1 1 1 1 1 1 1 1 1 ...
    ##  $ baker           : chr [1:1136] "Annetha" "David" "Edd" "Jasminder" ...
    ##  $ last_name       : chr [1:1136] "Mills" "Chambers" "Kimber" "Randhawa" ...
    ##  $ signature_bake  : chr [1:1136] "Light Jamaican Black Cakewith Strawberries and Cream" "Chocolate Orange Cake" "Caramel Cinnamon and Banana Cake" "Fresh Mango and Passion Fruit Hummingbird Cake" ...
    ##  $ technical       : num [1:1136] 2 3 1 NA 9 NA 8 NA 10 NA ...
    ##  $ show_stopper    : chr [1:1136] "Red, White & Blue Chocolate Cake with Cigarellos, Fresh Fruit, and Cream" "Black Forest Floor Gateauxwith Moulded Chocolate Leaves, Fallen Fruitand Chocolate Mushrooms Moulded from eggs" NA NA ...
    ##  $ result          : chr [1:1136] "IN" "IN" "IN" "IN" ...
    ##  $ baker_age       : num [1:1136] 30 31 24 45 25 44 37 31 51 48 ...
    ##  $ baker_occupation: chr [1:1136] "Midwife" "Entrepreneur" "Debt collector for Yorkshire Bank" "Assistant Credit Control Manager" ...
    ##  $ hometown        : chr [1:1136] "Essex" "Milton Keynes" "Bradford" "Birmingham" ...

After merging the datasets, check for completeness and correctness was
performed by using the anti_join function on each of the dataset with
the merged dataset. As shown above, each of the anti_join function
returned an empty dataframe, indicating that none of the data were
omitted.

The final dataset has 1136 rows and 11 columns. It includes the series,
episode, technical and baker age as numerical variables with baker
(first name), last name, signature bake, show stopper, occupation and
hometown as character variables.

## Create star baker table:

``` r
star_baker_table = merged_gbb_df |>
  filter(result %in% c("STAR BAKER", "WINNER")) |>
  filter(series %in% c(5:10)) |> 
  arrange(series, episode) |>
  select(episode, baker, series) |>
  pivot_wider(
    names_from = "series",
    values_from = "baker"  ) |>
  rename("series 5" = "5",
         "series 6" = "6",
         "series 7" = "7",
         "series 8" = "8",
         "series 9" = "9",
         "series 10" = "10") |>
  knitr::kable()

star_baker_table
```

| episode | series 5 | series 6 | series 7  | series 8 | series 9 | series 10 |
|--------:|:---------|:---------|:----------|:---------|:---------|:----------|
|       1 | Nancy    | Marie    | Jane      | Steven   | Manon    | Michelle  |
|       2 | Richard  | Ian      | Candice   | Steven   | Rahul    | Alice     |
|       3 | Luis     | Ian      | Tom       | Julia    | Rahul    | Michael   |
|       4 | Richard  | Ian      | Benjamina | Kate     | Dan      | Steph     |
|       5 | Kate     | Nadiya   | Candice   | Sophie   | Kim-Joy  | Steph     |
|       6 | Chetna   | Mat      | Tom       | Liam     | Briony   | Steph     |
|       7 | Richard  | Tamal    | Andrew    | Steven   | Kim-Joy  | Henry     |
|       8 | Richard  | Nadiya   | Candice   | Stacey   | Ruby     | Steph     |
|       9 | Richard  | Nadiya   | Andrew    | Sophie   | Ruby     | Alice     |
|      10 | Nancy    | Nadiya   | Candice   | Sophie   | Rahul    | David     |

### Were there any predictable overall winners? Any surprises?

The following contestants had the most titles for star baker before
episode 10 (the final episode) and were predicted to win:

- Richard (3 times for series 5)
- Either Ian or Nadiya (3 times each for series 6),
- Candice (3 times for series 7)
- Steven (3 times for series 8)
- Either Rahul, Kim-Joy or Ruby (2 times each for series 9)
- Steph (4 times for series 10).

As shown in the table, for series 6, 7 and 9, Nadiya, Candice and Rahul
won respectively. As a surprise, Nancy, Sophie, and David won series 5,
8, and 10 respectively.

## Import and tidy viewership dataset:

``` r
viewership_df =
  read_csv("./data/gbb_datasets/viewers.csv",
           na = "NA") |>
  janitor::clean_names()
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
head(viewership_df, 10)
```

    ## # A tibble: 10 × 11
    ##    episode series_1 series_2 series_3 series_4 series_5 series_6 series_7
    ##      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
    ##  1       1     2.24     3.1      3.85     6.6      8.51     11.6     13.6
    ##  2       2     3        3.53     4.6      6.65     8.79     11.6     13.4
    ##  3       3     3        3.82     4.53     7.17     9.28     12.0     13.0
    ##  4       4     2.6      3.6      4.71     6.82    10.2      12.4     13.3
    ##  5       5     3.03     3.83     4.61     6.95     9.95     12.4     13.1
    ##  6       6     2.75     4.25     4.82     7.32    10.1      12       13.1
    ##  7       7    NA        4.42     5.1      7.76    10.3      12.4     13.4
    ##  8       8    NA        5.06     5.35     7.41     9.02     11.1     13.3
    ##  9       9    NA       NA        5.7      7.41    10.7      12.6     13.4
    ## 10      10    NA       NA        6.74     9.45    13.5      15.0     15.9
    ## # ℹ 3 more variables: series_8 <dbl>, series_9 <dbl>, series_10 <dbl>

### What was the average viewership in Season 1? In Season 5?

The average viewership of seasons 1 and 5 were 2.77 and 10.04
respectively.
